---
title: "Part 2: Model Training and Optimization"
subtitle: "GPU-Accelerated Machine Learning Pipeline"
format:
  html:
    toc: true
    code-fold: show
    embed-resources: true
---

# Machine Learning Pipeline {#sec-ml-pipeline}

## Data Preparation

```{r prepare-ml-data}
#| label: prepare-ml-data
#| eval: false

cli::cli_h1("Machine Learning Data Preparation")

# Load full dataset
full_dataset <- read_csv("data/full_dataset_with_descriptors.csv")

# Select features for modeling
feature_cols <- c(
  # Lipinski descriptors
  "MW", "LogP", "HBD", "HBA", "TPSA", "nRotB", "nAtoms",
  # Fingerprints
  grep("^FP", names(full_dataset), value = TRUE)
)

# Prepare modeling dataset
model_data <- full_dataset %>%
  select(molecule_chembl_id, pIC50, bioactivity_class, all_of(feature_cols)) %>%
  drop_na()

cli::cli_alert_success(glue("Modeling dataset: {nrow(model_data)} samples, {length(feature_cols)} features"))

# Check for infinite values
model_data <- model_data %>%
  mutate(across(where(is.numeric), ~ifelse(is.infinite(.), NA, .))) %>%
  drop_na()

# Split into train/test (80/20)
set.seed(42)
train_index <- createDataPartition(model_data$pIC50, p = 0.8, list = FALSE)

train_data <- model_data[train_index, ]
test_data <- model_data[-train_index, ]

cli::cli_alert_info(glue("Training set: {nrow(train_data)} samples"))
cli::cli_alert_info(glue("Test set: {nrow(test_data)} samples"))

# Save train/test splits
write_csv(train_data, "data/train_data.csv")
write_csv(test_data, "data/test_data.csv")

check_memory("After data preparation")
```

## Random Forest with Ranger

```{r ranger-model}
#| label: ranger-model
#| eval: false

cli::cli_h2("Training Random Forest Model (Ranger)")

# Load data
train_data <- read_csv("data/train_data.csv")
test_data <- read_csv("data/test_data.csv")

# Prepare formula
feature_cols <- setdiff(names(train_data), c("molecule_chembl_id", "pIC50", "bioactivity_class"))

# Setup parallel processing for ranger
cores <- setup_parallel()

# Train Random Forest with progress
cli::cli_alert_info("Training Random Forest with optimized parameters...")


handlers("cli")

tic("Random Forest Training")

with_progress({
  p <- progressor(steps = 500)
  
  rf_model <- ranger(
    pIC50 ~ .,
    data = train_data %>% select(pIC50, all_of(feature_cols)),
    num.trees = 500,
    mtry = floor(sqrt(length(feature_cols))),
    importance = "permutation",
    num.threads = cores,
    verbose = FALSE,
    seed = 42,
    # Optimizations
    save.memory = TRUE,
    oob.error = TRUE
  )
  
  for(i in 1:500) {
    p(message = sprintf("Tree %d/500", i))
    Sys.sleep(0.01)  # Simulate progress
  }
})

toc()

# Model summary
cat("\n")
cli::cli_h3("Model Performance Summary")
cat(glue("
  Number of trees: {rf_model$num.trees}
  MTry: {rf_model$mtry}
  OOB Prediction Error (MSE): {round(rf_model$prediction.error, 4)}
  OOB R-squared: {round(rf_model$r.squared, 4)}
"))

# Make predictions on test set
cli::cli_alert_info("Making predictions on test set...")

test_predictions <- predict(rf_model, test_data)$predictions

# Calculate metrics
test_metrics <- data.frame(
  Actual = test_data$pIC50,
  Predicted = test_predictions,
  Residuals = test_data$pIC50 - test_predictions
)

mse <- mean(test_metrics$Residuals^2)
rmse <- sqrt(mse)
mae <- mean(abs(test_metrics$Residuals))
r_squared <- cor(test_metrics$Actual, test_metrics$Predicted)^2

cat("\n")
cli::cli_h3("Test Set Performance")
cat(glue("
  MSE: {round(mse, 4)}
  RMSE: {round(rmse, 4)}
  MAE: {round(mae, 4)}
  R²: {round(r_squared, 4)}
"))

# Save model and predictions
saveRDS(rf_model, "models/random_forest_model.rds")
write_csv(test_metrics, "results/rf_test_predictions.csv")

check_memory("After RF training")
```

## Hyperparameter Tuning with Grid Search

```{r rf-tuning}
#| label: rf-tuning
#| eval: false

cli::cli_h2("Hyperparameter Tuning with Grid Search")

# Load data
train_data <- read_csv("data/train_data.csv")
feature_cols <- setdiff(names(train_data), c("molecule_chembl_id", "pIC50", "bioactivity_class"))

# Define parameter grid
param_grid <- expand.grid(
  num.trees = c(300, 500, 700),
  mtry = c(10, 20, floor(sqrt(length(feature_cols))), 50),
  min.node.size = c(3, 5, 10)
)

cli::cli_alert_info(glue("Testing {nrow(param_grid)} parameter combinations"))

# Configure parallel plan for grid search to avoid conflicts
future::plan(multisession, workers = min(4, parallel::detectCores() - 1))
handlers("cli")  # Use local CLI handler

tic("Grid Search")

results <- with_progress({
  p <- progressor(steps = nrow(param_grid))
  
  furrr::future_map_dfr(1:nrow(param_grid), function(i) {
    params <- param_grid[i, ]
    p(message = sprintf("Config %d/%d: trees=%d, mtry=%d, nodesize=%d",
                        i, nrow(param_grid), params$num.trees, params$mtry, params$min.node.size))
    set.seed(42)
    folds <- createFolds(train_data$pIC50, k = 5)
    cv_scores <- purrr::map_dbl(folds, function(fold_idx) {
      train_fold <- train_data[-fold_idx, ] %>% select(pIC50, all_of(feature_cols))
      val_fold <- train_data[fold_idx, ] %>% select(pIC50, all_of(feature_cols))
      tryCatch({
        model <- ranger(
          pIC50 ~ .,
          data = train_fold,
          num.trees = params$num.trees,
          mtry = params$mtry,
          min.node.size = params$min.node.size,
          num.threads = 1,
          verbose = FALSE,
          save.memory = TRUE
        )
        preds <- predict(model, val_fold)$predictions
        sqrt(mean((val_fold$pIC50 - preds)^2))
      }, error = function(e) {
        cli::cli_alert_warning(sprintf("Error with model c%d: %s", i, e$message))
        NA
      })
    })
    data.frame(
      num.trees = params$num.trees,
      mtry = params$mtry,
      min.node.size = params$min.node.size,
      mean_cv_rmse = mean(cv_scores, na.rm = TRUE),
      sd_cv_rmse = sd(cv_scores, na.rm = TRUE)
    )
  }, .options = furrr_options(seed = TRUE))
})

toc()

# Reset to sequential plan after parallel grid search
future::plan(sequential)

# Find best hyperparameters
best_params <- results %>%
  arrange(mean_cv_rmse) %>%
  slice(1)

cli::cli_alert_success("Best parameters found!")
print(knitr::kable(best_params, digits = 4, caption = "Best Hyperparameters"))

# Train final model with best params on full dataset, with tryCatch
cli::cli_alert_info("Training final model with best parameters...")
tryCatch({
  cores <- parallel::detectCores() - 1
  final_rf_model <- ranger(
    pIC50 ~ .,
    data = train_data %>% select(pIC50, all_of(feature_cols)),
    num.trees = best_params$num.trees,
    mtry = best_params$mtry,
    min.node.size = best_params$min.node.size,
    importance = "permutation",
    num.threads = cores,
    verbose = TRUE
  )
  saveRDS(final_rf_model, "models/final_rf_model_tuned.rds")
}, error = function(e) {
  cli::cli_alert_danger(sprintf("Error training final model: %s", e$message))
})

# Save tuning results
write_csv(results, "results/hyperparameter_tuning_results.csv")
check_memory("After hyperparameter tuning")
```

## XGBoost Model

```{r xgboost-model}
#| label: xgboost-model
#| eval: false

cli::cli_h2("Training XGBoost Model")

# Load data
train_data <- read_csv("data/train_data.csv")
test_data <- read_csv("data/test_data.csv")
feature_cols <- setdiff(names(train_data), c("molecule_chembl_id", "pIC50", "bioactivity_class"))

# Prepare matrices
dtrain <- xgb.DMatrix(
  data = as.matrix(train_data %>% select(all_of(feature_cols))),
  label = train_data$pIC50
)
dtest <- xgb.DMatrix(
  data = as.matrix(test_data %>% select(all_of(feature_cols))),
  label = test_data$pIC50
)

# Setup parameters
params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse",
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8,
  min_child_weight = 3,
  nthread = parallel::detectCores() - 1
)

cli::cli_alert_info("Training XGBoost model...")
tic("XGBoost Training")

xgb_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 500,
  watchlist = list(train = dtrain, test = dtest),
  early_stopping_rounds = 50,
  verbose = 1,  # Enable built-in progress output
  print_every_n = 10  # Print every 10 rounds
)

toc()

# Make predictions
xgb_predictions <- predict(xgb_model, dtest)

# Calculate metrics
xgb_metrics <- data.frame(
  Actual = test_data$pIC50,
  Predicted = xgb_predictions,
  Residuals = test_data$pIC50 - xgb_predictions
)

xgb_mse <- mean(xgb_metrics$Residuals^2)
xgb_rmse <- sqrt(xgb_mse)
xgb_mae <- mean(abs(xgb_metrics$Residuals))
xgb_r2 <- cor(xgb_metrics$Actual, xgb_metrics$Predicted)^2

cat("\n")
cli::cli_h3("XGBoost Test Performance")
cat(glue("
  MSE: {round(xgb_mse, 4)}
  RMSE: {round(xgb_rmse, 4)}
  MAE: {round(xgb_mae, 4)}
  R²: {round(xgb_r2, 4)}
"))

xgb.save(xgb_model, "models/xgboost_model.json")
write_csv(xgb_metrics, "results/xgb_test_predictions.csv")
check_memory("After XGBoost training")

```

## Deep Learning with Torch (GPU Accelerated)

```{r torch-model}
#| label: torch-model
#| eval: false

cli::cli_h2("Training Deep Neural Network with Torch")

# Check GPU availability
gpu_available <- check_gpu()
device <- if(gpu_available) torch_device("cuda") else torch_device("cpu")

# Load and prepare data
train_data <- read_csv("data/train_data.csv")
test_data <- read_csv("data/test_data.csv")

feature_cols <- setdiff(names(train_data), c("molecule_chembl_id", "pIC50", "bioactivity_class"))

# Normalize features
train_features <- as.matrix(train_data %>% select(all_of(feature_cols)))
test_features <- as.matrix(test_data %>% select(all_of(feature_cols)))

mean_vals <- colMeans(train_features)
sd_vals <- apply(train_features, 2, sd)

train_features_norm <- scale(train_features, center = mean_vals, scale = sd_vals)
test_features_norm <- scale(test_features, center = mean_vals, scale = sd_vals)

# Convert to tensors
train_x <- torch_tensor(train_features_norm, dtype = torch_float())$to(device = device)
train_y <- torch_tensor(train_data$pIC50, dtype = torch_float())$unsqueeze(2)$to(device = device)

test_x <- torch_tensor(test_features_norm, dtype = torch_float())$to(device = device)
test_y <- torch_tensor(test_data$pIC50, dtype = torch_float())$unsqueeze(2)$to(device = device)

# Define neural network
nn_model <- nn_module(
  "DrugDiscoveryNN",
  
  initialize = function(input_size) {
    self$fc1 <- nn_linear(input_size, 512)
    self$bn1 <- nn_batch_norm1d(512)
    self$dropout1 <- nn_dropout(0.3)
    
    self$fc2 <- nn_linear(512, 256)
    self$bn2 <- nn_batch_norm1d(256)
    self$dropout2 <- nn_dropout(0.3)
    
    self$fc3 <- nn_linear(256, 128)
    self$bn3 <- nn_batch_norm1d(128)
    self$dropout3 <- nn_dropout(0.2)
    
    self$fc4 <- nn_linear(128, 64)
    self$bn4 <- nn_batch_norm1d(64)
    
    self$output <- nn_linear(64, 1)
  },
  
  forward = function(x) {
    x %>%
      self$fc1() %>%
      self$bn1() %>%
      nnf_relu() %>%
      self$dropout1() %>%
      
      self$fc2() %>%
      self$bn2() %>%
      nnf_relu() %>%
      self$dropout2() %>%
      
      self$fc3() %>%
      self$bn3() %>%
      nnf_relu() %>%
      self$dropout3() %>%
      
      self$fc4() %>%
      self$bn4() %>%
      nnf_relu() %>%
      
      self$output()
  }
)

# Initialize model
model <- nn_model(input_size = ncol(train_features))
model$to(device = device)

# Loss and optimizer
criterion <- nn_mse_loss()
optimizer <- optim_adam(model$parameters, lr = 0.001)

# Training loop
epochs <- 100
batch_size <- 64

cli::cli_alert_info(glue("Training on {device}"))


handlers("cli")

tic("Neural Network Training")

training_history <- with_progress({
  p <- progressor(steps = epochs)
  
  map_dfr(1:epochs, function(epoch) {
    model$train()
    
    # Mini-batch training
    n_batches <- ceiling(nrow(train_features) / batch_size)
    epoch_loss <- 0
    
    for(i in 1:n_batches) {
      start_idx <- (i - 1) * batch_size + 1
      end_idx <- min(i * batch_size, nrow(train_features))
      
      batch_x <- train_x[start_idx:end_idx, ]
      batch_y <- train_y[start_idx:end_idx, ]
      
      # Forward pass
      optimizer$zero_grad()
      predictions <- model(batch_x)
      loss <- criterion(predictions, batch_y)
      
      # Backward pass
      loss$backward()
      optimizer$step()
      
      epoch_loss <- epoch_loss + loss$item()
    }
    
    avg_loss <- epoch_loss / n_batches
    
    # Validation
    model$eval()
    with_no_grad({
      val_pred <- model(test_x)
      val_loss <- criterion(val_pred, test_y)$item()
    })
    
    p(message = sprintf(
      "Epoch %d/%d - Loss: %.4f - Val Loss: %.4f",
      epoch, epochs, avg_loss, val_loss
    ))
    
    data.frame(
      epoch = epoch,
      train_loss = avg_loss,
      val_loss = val_loss
    )
  })
})

toc()

# Final predictions
model$eval()
with_no_grad({
  out <- model(test_x)
  out_cpu <- out$to(device="cpu")
  arr <- as_array(out_cpu)
  nn_predictions <- arr[, 1]
})


# Calculate metrics
nn_metrics <- data.frame(
  Actual = test_data$pIC50,
  Predicted = nn_predictions,
  Residuals = test_data$pIC50 - nn_predictions
)

nn_mse <- mean(nn_metrics$Residuals^2)
nn_rmse <- sqrt(nn_mse)
nn_mae <- mean(abs(nn_metrics$Residuals))
nn_r2 <- cor(nn_metrics$Actual, nn_metrics$Predicted)^2

cat("\n")
cli::cli_h3("Neural Network Test Performance")
cat(glue("
  MSE: {round(nn_mse, 4)}
  RMSE: {round(nn_rmse, 4)}
  MAE: {round(nn_mae, 4)}
  R²: {round(nn_r2, 4)}
"))

# Save model and results
torch_save(model, "models/torch_nn_model.pt")
write_csv(nn_metrics, "results/nn_test_predictions.csv")
write_csv(training_history, "results/nn_training_history.csv")

check_memory("After neural network training")
```

## Model Comparison

```{r model-comparison}
#| label: model-comparison
#| eval: false

cli::cli_h2("Comparing All Models")

# Load all predictions
rf_pred <- read_csv("results/rf_test_predictions.csv")
xgb_pred <- read_csv("results/xgb_test_predictions.csv")
nn_pred <- read_csv("results/nn_test_predictions.csv")

# Calculate metrics for all models
calc_metrics <- function(actual, predicted, model_name) {
  residuals <- actual - predicted
  data.frame(
    Model = model_name,
    MSE = mean(residuals^2),
    RMSE = sqrt(mean(residuals^2)),
    MAE = mean(abs(residuals)),
    R_squared = cor(actual, predicted)^2,
    stringsAsFactors = FALSE
  )
}

comparison <- bind_rows(
  calc_metrics(rf_pred$Actual, rf_pred$Predicted, "Random Forest"),
  calc_metrics(xgb_pred$Actual, xgb_pred$Predicted, "XGBoost"),
  calc_metrics(nn_pred$Actual, nn_pred$Predicted, "Neural Network")
) %>%
  arrange(RMSE)

print(knitr::kable(comparison, digits = 4, caption = "Model Performance Comparison"))
write_csv(comparison, "results/model_comparison.csv")

# --- Plotly Visualizations ---

# 1. Bar Chart: Performance Metrics Comparison
metrics_long <- comparison %>%
  pivot_longer(cols = c(MSE, RMSE, MAE, R_squared), 
               names_to = "Metric", 
               values_to = "Value")

fig_metrics <- plot_ly(metrics_long, 
                       x = ~Model, 
                       y = ~Value, 
                       color = ~Metric, 
                       type = "bar",
                       text = ~round(Value, 4),
                       textposition = "auto") %>%
  layout(title = "Model Performance Metrics Comparison",
         xaxis = list(title = "Model"),
         yaxis = list(title = "Metric Value"),
         barmode = "group",
         hovermode = "closest")

fig_metrics

# 2. Scatter Plot: Actual vs Predicted for All Models (FIXED)
all_predictions <- bind_rows(
  rf_pred %>% mutate(Model = "Random Forest"),
  xgb_pred %>% mutate(Model = "XGBoost"),
  nn_pred %>% mutate(Model = "Neural Network")
)

# Calculate perfect prediction line separately
pred_range <- c(min(all_predictions$Actual, na.rm = TRUE), 
                max(all_predictions$Actual, na.rm = TRUE))

fig_scatter <- plot_ly() %>%
  # Add scatter points for each model
  add_trace(data = all_predictions,
            x = ~Actual, 
            y = ~Predicted, 
            color = ~Model,
            type = "scatter",
            mode = "markers",
            marker = list(size = 8, opacity = 0.7),
            text = ~paste("Actual:", round(Actual, 2), 
                         "<br>Predicted:", round(Predicted, 2)),
            hoverinfo = "text") %>%
  # Add diagonal perfect prediction line separately
  add_trace(x = pred_range,
            y = pred_range,
            mode = "lines",
            line = list(color = "black", dash = "dash", width = 2),
            name = "Perfect Prediction",
            hoverinfo = "skip") %>%
  layout(title = "Actual vs Predicted pIC50 (All Models)",
         xaxis = list(title = "Actual pIC50"),
         yaxis = list(title = "Predicted pIC50"),
         hovermode = "closest")

fig_scatter

# 3. Residual Plot (FIXED)
fig_residuals <- plot_ly() %>%
  # Add residual points
  add_trace(data = all_predictions,
            x = ~Predicted, 
            y = ~Residuals, 
            color = ~Model,
            type = "scatter",
            mode = "markers",
            marker = list(size = 8, opacity = 0.7),
            hoverinfo = "text",
            text = ~paste("Predicted:", round(Predicted, 2),
                         "<br>Residual:", round(Residuals, 2))) %>%
  # Add zero error line separately
  add_trace(x = range(all_predictions$Predicted, na.rm = TRUE),
            y = c(0, 0),
            mode = "lines",
            line = list(color = "red", dash = "dash", width = 2),
            name = "Zero Error",
            hoverinfo = "skip") %>%
  layout(title = "Residual Plot (All Models)",
         xaxis = list(title = "Predicted pIC50"),
         yaxis = list(title = "Residuals"),
         hovermode = "closest")

fig_residuals

# 4. R-squared Comparison Bar Chart (FIXED)
fig_r2 <- plot_ly(comparison,
                   x = ~Model,
                   y = ~R_squared,
                   type = "bar",
                   marker = list(color = c("lightblue", "lightgreen", "coral")),
                   text = ~round(R_squared, 4),
                   textposition = "auto",
                   hoverinfo = "text",
                   hovertext = ~paste("Model:", Model, 
                                     "<br>R²:", round(R_squared, 4))) %>%
  layout(title = "R-squared Comparison Across Models",
         xaxis = list(title = "Model"),
         yaxis = list(title = "R²", range = c(0, 1)))

fig_r2

cli::cli_alert_success("Model comparison plots generated!")
```

To be continued in Part 3 (Shiny Dashboard)...
