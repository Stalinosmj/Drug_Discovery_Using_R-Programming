---
title: "Part 2: Model Training and Optimization"
subtitle: "GPU-Accelerated Machine Learning Pipeline"
format:
  html:
    toc: true
    code-fold: show
    embed-resources: true
---

# Machine Learning Pipeline {#sec-ml-pipeline}

## Data Preparation

```{r prepare-ml-data}
#| label: prepare-ml-data
#| eval: false

cli::cli_h1("Machine Learning Data Preparation")

# Load full dataset
full_dataset <- read_csv("data/full_dataset_with_descriptors.csv")

# Select features for modeling
feature_cols <- c(
  # Lipinski descriptors
  "MW", "LogP", "HBD", "HBA", "TPSA", "nRotB", "nAtoms",
  # Fingerprints
  grep("^FP", names(full_dataset), value = TRUE)
)

# Prepare modeling dataset
model_data <- full_dataset %>%
  select(molecule_chembl_id, pIC50, bioactivity_class, all_of(feature_cols)) %>%
  drop_na()

cli::cli_alert_success(glue("Modeling dataset: {nrow(model_data)} samples, {length(feature_cols)} features"))

# Check for infinite values
model_data <- model_data %>%
  mutate(across(where(is.numeric), ~ifelse(is.infinite(.), NA, .))) %>%
  drop_na()

# Split into train/test (80/20)
set.seed(42)
train_index <- createDataPartition(model_data$pIC50, p = 0.8, list = FALSE)

train_data <- model_data[train_index, ]
test_data <- model_data[-train_index, ]

cli::cli_alert_info(glue("Training set: {nrow(train_data)} samples"))
cli::cli_alert_info(glue("Test set: {nrow(test_data)} samples"))

# Save train/test splits
write_csv(train_data, "data/train_data.csv")
write_csv(test_data, "data/test_data.csv")

check_memory("After data preparation")
```

## Random Forest with Ranger

```{r ranger-model}
#| label: ranger-model
#| eval: false

cli::cli_h2("Training Random Forest Model (Ranger)")

# Load data
train_data <- read_csv("data/train_data.csv")
test_data <- read_csv("data/test_data.csv")

# Prepare formula
feature_cols <- setdiff(names(train_data), c("molecule_chembl_id", "pIC50", "bioactivity_class"))

# Setup parallel processing for ranger
cores <- setup_parallel()

# Train Random Forest with progress
cli::cli_alert_info("Training Random Forest with optimized parameters...")

handlers(global = TRUE)
handlers("cli")

tic("Random Forest Training")

with_progress({
  p <- progressor(steps = 500)
  
  rf_model <- ranger(
    pIC50 ~ .,
    data = train_data %>% select(pIC50, all_of(feature_cols)),
    num.trees = 500,
    mtry = floor(sqrt(length(feature_cols))),
    importance = "permutation",
    num.threads = cores,
    verbose = FALSE,
    seed = 42,
    # Optimizations
    save.memory = TRUE,
    oob.error = TRUE
  )
  
  for(i in 1:500) {
    p(message = sprintf("Tree %d/500", i))
    Sys.sleep(0.01)  # Simulate progress
  }
})

toc()

# Model summary
cat("\n")
cli::cli_h3("Model Performance Summary")
cat(glue("
  Number of trees: {rf_model$num.trees}
  MTry: {rf_model$mtry}
  OOB Prediction Error (MSE): {round(rf_model$prediction.error, 4)}
  OOB R-squared: {round(rf_model$r.squared, 4)}
"))

# Make predictions on test set
cli::cli_alert_info("Making predictions on test set...")

test_predictions <- predict(rf_model, test_data)$predictions

# Calculate metrics
test_metrics <- data.frame(
  Actual = test_data$pIC50,
  Predicted = test_predictions,
  Residuals = test_data$pIC50 - test_predictions
)

mse <- mean(test_metrics$Residuals^2)
rmse <- sqrt(mse)
mae <- mean(abs(test_metrics$Residuals))
r_squared <- cor(test_metrics$Actual, test_metrics$Predicted)^2

cat("\n")
cli::cli_h3("Test Set Performance")
cat(glue("
  MSE: {round(mse, 4)}
  RMSE: {round(rmse, 4)}
  MAE: {round(mae, 4)}
  R²: {round(r_squared, 4)}
"))

# Save model and predictions
saveRDS(rf_model, "models/random_forest_model.rds")
write_csv(test_metrics, "results/rf_test_predictions.csv")

check_memory("After RF training")
```

## Hyperparameter Tuning with Grid Search

```{r rf-tuning}
#| label: rf-tuning
#| eval: false

cli::cli_h2("Hyperparameter Tuning with Grid Search")

# Load data
train_data <- read_csv("data/train_data.csv")
feature_cols <- setdiff(names(train_data), c("molecule_chembl_id", "pIC50", "bioactivity_class"))

# Define parameter grid
param_grid <- expand.grid(
  num.trees = c(300, 500, 700),
  mtry = c(10, 20, floor(sqrt(length(feature_cols))), 50),
  min.node.size = c(3, 5, 10)
)

cli::cli_alert_info(glue("Testing {nrow(param_grid)} parameter combinations"))

# Setup parallel processing
cores <- setup_parallel()

# Perform grid search with progress
handlers(global = TRUE)
handlers("cli")

tic("Grid Search")

results <- with_progress({
  p <- progressor(steps = nrow(param_grid))
  
  future_map_dfr(1:nrow(param_grid), function(i) {
    params <- param_grid[i, ]
    
    p(message = sprintf(
      "Config %d/%d: trees=%d, mtry=%d, nodesize=%d",
      i, nrow(param_grid), params$num.trees, params$mtry, params$min.node.size
    ))
    
    # 5-fold CV
    set.seed(42)
    folds <- createFolds(train_data$pIC50, k = 5)
    
    cv_scores <- map_dbl(folds, function(fold_idx) {
      train_fold <- train_data[-fold_idx, ] %>% select(pIC50, all_of(feature_cols))
      val_fold <- train_data[fold_idx, ] %>% select(pIC50, all_of(feature_cols))
      
      model <- ranger(
        pIC50 ~ .,
        data = train_fold,
        num.trees = params$num.trees,
        mtry = params$mtry,
        min.node.size = params$min.node.size,
        num.threads = 1,
        verbose = FALSE,
        save.memory = TRUE
      )
      
      preds <- predict(model, val_fold)$predictions
      sqrt(mean((val_fold$pIC50 - preds)^2))
    })
    
    data.frame(
      num.trees = params$num.trees,
      mtry = params$mtry,
      min.node.size = params$min.node.size,
      mean_cv_rmse = mean(cv_scores),
      sd_cv_rmse = sd(cv_scores)
    )
  }, .options = furrr_options(seed = TRUE))
})

toc()

# Find best parameters
best_params <- results %>%
  arrange(mean_cv_rmse) %>%
  slice(1)

cli::cli_alert_success("Best parameters found!")
print(knitr::kable(best_params, digits = 4, caption = "Best Hyperparameters"))

# Train final model with best parameters
cli::cli_alert_info("Training final model with best parameters...")

final_rf_model <- ranger(
  pIC50 ~ .,
  data = train_data %>% select(pIC50, all_of(feature_cols)),
  num.trees = best_params$num.trees,
  mtry = best_params$mtry,
  min.node.size = best_params$min.node.size,
  importance = "permutation",
  num.threads = cores,
  verbose = TRUE
)

# Save tuning results and final model
write_csv(results, "results/hyperparameter_tuning_results.csv")
saveRDS(final_rf_model, "models/final_rf_model_tuned.rds")

check_memory("After hyperparameter tuning")
```

## XGBoost Model

```{r xgboost-model}
#| label: xgboost-model
#| eval: false

cli::cli_h2("Training XGBoost Model")

# Load data
train_data <- read_csv("data/train_data.csv")
test_data <- read_csv("data/test_data.csv")

feature_cols <- setdiff(names(train_data), c("molecule_chembl_id", "pIC50", "bioactivity_class"))

# Prepare matrices
dtrain <- xgb.DMatrix(
  data = as.matrix(train_data %>% select(all_of(feature_cols))),
  label = train_data$pIC50
)

dtest <- xgb.DMatrix(
  data = as.matrix(test_data %>% select(all_of(feature_cols))),
  label = test_data$pIC50
)

# Setup parameters
params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse",
  max_depth = 6,
  eta = 0.1,
  subsample = 0.8,
  colsample_bytree = 0.8,
  min_child_weight = 3,
  nthread = parallel::detectCores() - 1
)

# Train with progress
cli::cli_alert_info("Training XGBoost model...")

handlers(global = TRUE)
handlers("cli")

tic("XGBoost Training")

with_progress({
  p <- progressor(steps = 500)
  
  # Custom callback for progress
  progress_callback <- function(env) {
    p(message = sprintf("Round %d/500 - RMSE: %.4f", 
                       env$iteration, env$evaluation_log$train_rmse[env$iteration]))
  }
  
  xgb_model <- xgb.train(
    params = params,
    data = dtrain,
    nrounds = 500,
    watchlist = list(train = dtrain, test = dtest),
    early_stopping_rounds = 50,
    verbose = 0,
    callbacks = list(progress_callback)
  )
})

toc()

# Make predictions
xgb_predictions <- predict(xgb_model, dtest)

# Calculate metrics
xgb_metrics <- data.frame(
  Actual = test_data$pIC50,
  Predicted = xgb_predictions,
  Residuals = test_data$pIC50 - xgb_predictions
)

xgb_mse <- mean(xgb_metrics$Residuals^2)
xgb_rmse <- sqrt(xgb_mse)
xgb_mae <- mean(abs(xgb_metrics$Residuals))
xgb_r2 <- cor(xgb_metrics$Actual, xgb_metrics$Predicted)^2

cat("\n")
cli::cli_h3("XGBoost Test Performance")
cat(glue("
  MSE: {round(xgb_mse, 4)}
  RMSE: {round(xgb_rmse, 4)}
  MAE: {round(xgb_mae, 4)}
  R²: {round(xgb_r2, 4)}
"))

# Save model and predictions
xgb.save(xgb_model, "models/xgboost_model.json")
write_csv(xgb_metrics, "results/xgb_test_predictions.csv")

check_memory("After XGBoost training")
```

## Deep Learning with Torch (GPU Accelerated)

```{r torch-model}
#| label: torch-model
#| eval: false

cli::cli_h2("Training Deep Neural Network with Torch")

# Check GPU availability
gpu_available <- check_gpu()
device <- if(gpu_available) torch_device("cuda") else torch_device("cpu")

# Load and prepare data
train_data <- read_csv("data/train_data.csv")
test_data <- read_csv("data/test_data.csv")

feature_cols <- setdiff(names(train_data), c("molecule_chembl_id", "pIC50", "bioactivity_class"))

# Normalize features
train_features <- as.matrix(train_data %>% select(all_of(feature_cols)))
test_features <- as.matrix(test_data %>% select(all_of(feature_cols)))

mean_vals <- colMeans(train_features)
sd_vals <- apply(train_features, 2, sd)

train_features_norm <- scale(train_features, center = mean_vals, scale = sd_vals)
test_features_norm <- scale(test_features, center = mean_vals, scale = sd_vals)

# Convert to tensors
train_x <- torch_tensor(train_features_norm, dtype = torch_float())$to(device = device)
train_y <- torch_tensor(train_data$pIC50, dtype = torch_float())$unsqueeze(2)$to(device = device)

test_x <- torch_tensor(test_features_norm, dtype = torch_float())$to(device = device)
test_y <- torch_tensor(test_data$pIC50, dtype = torch_float())$unsqueeze(2)$to(device = device)

# Define neural network
nn_model <- nn_module(
  "DrugDiscoveryNN",
  
  initialize = function(input_size) {
    self$fc1 <- nn_linear(input_size, 512)
    self$bn1 <- nn_batch_norm1d(512)
    self$dropout1 <- nn_dropout(0.3)
    
    self$fc2 <- nn_linear(512, 256)
    self$bn2 <- nn_batch_norm1d(256)
    self$dropout2 <- nn_dropout(0.3)
    
    self$fc3 <- nn_linear(256, 128)
    self$bn3 <- nn_batch_norm1d(128)
    self$dropout3 <- nn_dropout(0.2)
    
    self$fc4 <- nn_linear(128, 64)
    self$bn4 <- nn_batch_norm1d(64)
    
    self$output <- nn_linear(64, 1)
  },
  
  forward = function(x) {
    x %>%
      self$fc1() %>%
      self$bn1() %>%
      nnf_relu() %>%
      self$dropout1() %>%
      
      self$fc2() %>%
      self$bn2() %>%
      nnf_relu() %>%
      self$dropout2() %>%
      
      self$fc3() %>%
      self$bn3() %>%
      nnf_relu() %>%
      self$dropout3() %>%
      
      self$fc4() %>%
      self$bn4() %>%
      nnf_relu() %>%
      
      self$output()
  }
)

# Initialize model
model <- nn_model(input_size = ncol(train_features))
model$to(device = device)

# Loss and optimizer
criterion <- nn_mse_loss()
optimizer <- optim_adam(model$parameters, lr = 0.001)

# Training loop
epochs <- 100
batch_size <- 64

cli::cli_alert_info(glue("Training on {device}"))

handlers(global = TRUE)
handlers("cli")

tic("Neural Network Training")

training_history <- with_progress({
  p <- progressor(steps = epochs)
  
  map_dfr(1:epochs, function(epoch) {
    model$train()
    
    # Mini-batch training
    n_batches <- ceiling(nrow(train_features) / batch_size)
    epoch_loss <- 0
    
    for(i in 1:n_batches) {
      start_idx <- (i - 1) * batch_size + 1
      end_idx <- min(i * batch_size, nrow(train_features))
      
      batch_x <- train_x[start_idx:end_idx, ]
      batch_y <- train_y[start_idx:end_idx, ]
      
      # Forward pass
      optimizer$zero_grad()
      predictions <- model(batch_x)
      loss <- criterion(predictions, batch_y)
      
      # Backward pass
      loss$backward()
      optimizer$step()
      
      epoch_loss <- epoch_loss + loss$item()
    }
    
    avg_loss <- epoch_loss / n_batches
    
    # Validation
    model$eval()
    with_no_grad({
      val_pred <- model(test_x)
      val_loss <- criterion(val_pred, test_y)$item()
    })
    
    p(message = sprintf(
      "Epoch %d/%d - Loss: %.4f - Val Loss: %.4f",
      epoch, epochs, avg_loss, val_loss
    ))
    
    data.frame(
      epoch = epoch,
      train_loss = avg_loss,
      val_loss = val_loss
    )
  })
})

toc()

# Final predictions
model$eval()
with_no_grad({
  nn_predictions <- model(test_x)$to(device = "cpu")$numpy()[, 1]
})

# Calculate metrics
nn_metrics <- data.frame(
  Actual = test_data$pIC50,
  Predicted = nn_predictions,
  Residuals = test_data$pIC50 - nn_predictions
)

nn_mse <- mean(nn_metrics$Residuals^2)
nn_rmse <- sqrt(nn_mse)
nn_mae <- mean(abs(nn_metrics$Residuals))
nn_r2 <- cor(nn_metrics$Actual, nn_metrics$Predicted)^2

cat("\n")
cli::cli_h3("Neural Network Test Performance")
cat(glue("
  MSE: {round(nn_mse, 4)}
  RMSE: {round(nn_rmse, 4)}
  MAE: {round(nn_mae, 4)}
  R²: {round(nn_r2, 4)}
"))

# Save model and results
torch_save(model, "models/torch_nn_model.pt")
write_csv(nn_metrics, "results/nn_test_predictions.csv")
write_csv(training_history, "results/nn_training_history.csv")

check_memory("After neural network training")
```

## Model Comparison

```{r model-comparison}
#| label: model-comparison
#| eval: false

cli::cli_h2("Comparing All Models")

# Load all predictions
rf_pred <- read_csv("results/rf_test_predictions.csv")
xgb_pred <- read_csv("results/xgb_test_predictions.csv")
nn_pred <- read_csv("results/nn_test_predictions.csv")

# Calculate metrics for all models
calc_metrics <- function(actual, predicted, model_name) {
  residuals <- actual - predicted
  data.frame(
    Model = model_name,
    MSE = mean(residuals^2),
    RMSE = sqrt(mean(residuals^2)),
    MAE = mean(abs(residuals)),
    R_squared = cor(actual, predicted)^2,
    stringsAsFactors = FALSE
  )
}

comparison <- bind_rows(
  calc_metrics(rf_pred$Actual, rf_pred$Predicted, "Random Forest"),
  calc_metrics(xgb_pred$Actual, xgb_pred$Predicted, "XGBoost"),
  calc_metrics(nn_pred$Actual, nn_pred$Predicted, "Neural Network")
) %>%
  arrange(RMSE)

print(knitr::kable(comparison, digits = 4, caption = "Model Performance Comparison"))

# Save comparison
write_csv(comparison, "results/model_comparison.csv")
```

To be continued in Part 3 (Shiny Dashboard)...
